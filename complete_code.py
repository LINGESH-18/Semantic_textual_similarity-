# -*- coding: utf-8 -*-
"""complete_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OZqsiks6kqAOhB9scOnsw52465GfmMAW

# ***Natural language Processing , Pytorch , Tensorflow***

**Importing the library**
"""

import pandas as pd
import numpy as np

# For visualizing the dataset

import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split
import seaborn as sns

# For replacing the symbol that doesn't need 
import re

# Machine learning model
from sklearn.naive_bayes import GaussianNB

# Evaluating the model

from sklearn.metrics import confusion_matrix,accuracy_score



#  Compute Semantic Textual Similarity between paras using Pytorch and SentenceTransformers

from sentence_transformers import SentenceTransformer, util
model=SentenceTransformer('stsb-roberta-large')

#  Natural language processing , For Bag of words , Reducing the sparse matrix dimensinality , stemming

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# To avoid warning 

from warnings import filterwarnings
filterwarnings('ignore')

from IPython.display import display

"""**Reading csv**"""

data=pd.read_csv('Text_Similarity_Dataset.csv')
data.head(2)

"""**Data description**"""

# information of the dataset

data.info()

# Statistical description 

data.describe(include='object')

# Dataset visualization

sns.pairplot(data)

# Getting the bag of words

cv=CountVectorizer()
text1_array=cv.fit_transform(data['text1']).toarray()
x=text1_array

text1=data['text1'].values.tolist()
text2=data['text2'].values.tolist()

"""**Tensorflow and pytorch**

# Finding the sts value

Getting the sts value by **Pytorch** , SentenceTransformer
"""

sts=[]

for i in range(0,len(text1)):

    t1=re.sub('[^a-zA-Z]',' ',text1[i])
    t1=t1.lower()
    t2=re.sub('[^a-zA-Z]',' ',text2[i])
    t2=t2.lower()
    
    # encode Text to get their embeddings 
    
    embedding1 = model.encode(t1, convert_to_tensor=True)
    embedding2 = model.encode(t2, convert_to_tensor=True)
    
    # compute similarity scores of two embeddings using pytorch
    
    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)

    # Appending the similarity scores

    if cosine_scores.item()<=1:
        sts.append(round(cosine_scores.item()))
    else:
        sts.append(1)

data['Sts_Score']=sts

y=data['Sts_Score'].values

"""**Training testing Dataset**"""

xtr,xte,ytr,yte=train_test_split(x,y,test_size=0.2)

"""### **Naive bayes**"""

# creating a model and fitting the tained datas
classifier=GaussianNB()
classifier=classifier.fit(xtr,ytr)

"""**Prediction**"""

#predicting the test result
y_pred=classifier.predict(xte)

"""**Confusion matrix and Accuracy score**"""

print(confusion_matrix(y_pred,yte))

print(accuracy_score(yte,y_pred))

print(data['Sts_Score'].value_counts())

"""# **Conclusion:**
#####       From the observation the classification model developed with accuracy rate of **100% . The Sts** dissimilarity**(0)** between the  paragraph are **3725** , the similarity**(1)** between the paragraph are **298** . There is **no missing values** in the dataset.

# Why ? 
  **PyTorch and Tensorflow are among the most popular libraries for deep learning . Similarly to the way human brains process information, deep learning structures algorithms into layers creating deep artificial neural networks, which it can learn and make decisions on its own**

# Why not ?
   **On evaluating this model with other unsupervised machine learning the Naive bayes Model has the highest accuacy ~ (100%) Other machine learning model got less accuracy score**
"""